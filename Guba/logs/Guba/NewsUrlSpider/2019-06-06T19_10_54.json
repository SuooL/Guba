{
    "log_path": "/Users/suool/PycharmProjects/Guba/logs/Guba/NewsUrlSpider/2019-06-06T19_10_54.log",
    "json_path": "/Users/suool/PycharmProjects/Guba/logs/Guba/NewsUrlSpider/2019-06-06T19_10_54.json",
    "json_url": "http://127.0.0.1:6800/logs/Guba/NewsUrlSpider/2019-06-06T19_10_54.json",
    "size": 10365,
    "position": 10365,
    "status": "ok",
    "_head": 100,
    "pages": 1523,
    "items": 0,
    "first_log_time": "2019-06-06 19:11:02",
    "latest_log_time": "2019-06-06 19:50:46",
    "runtime": "0:39:44",
    "shutdown_reason": "Received SIGTERM",
    "finish_reason": "shutdown",
    "last_update_time": "2019-06-06 19:51:25",
    "head": "2019-06-06 19:11:02 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: Guba)\n2019-06-06 19:11:02 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.6.0-x86_64-i386-64bit\n2019-06-06 19:11:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'Guba', 'CONCURRENT_REQUESTS': 8, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'logs/Guba/NewsUrlSpider/2019-06-06T19_10_54.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'Guba.spiders', 'ROBOTSTXT_OBEY': True, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['Guba.spiders']}\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2019-06-06 19:11:02 [NewsUrlSpider] INFO: Reading start URLs from redis key 'Guba:start_urls' (batch size: 8, encoding: utf-8\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'Guba.middlewares.RandomUserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled item pipelines:\n['Guba.pipelines.GubaPipeline']\n2019-06-06 19:11:02 [scrapy.core.engine] INFO: Spider opened\n\n2019-06-06 19:11:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:12:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:13:02 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 9 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:14:02 [scrapy.extensions.logstats] INFO: Crawled 54 pages (at 45 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:15:02 [scrapy.extensions.logstats] INFO: Crawled 105 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:16:02 [scrapy.extensions.logstats] INFO: Crawled 154 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:17:02 [scrapy.extensions.logstats] INFO: Crawled 204 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:18:02 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:19:02 [scrapy.extensions.logstats] INFO: Crawled 301 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:20:02 [scrapy.extensions.logstats] INFO: Crawled 349 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:21:02 [scrapy.extensions.logstats] INFO: Crawled 395 pages (at 46 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:22:02 [scrapy.extensions.logstats] INFO: Crawled 445 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n2019-06-06 19:22:42 [scrapy.core.scraper] ERROR: Spider error processing <GET http://guba.eastmoney.com/list,600578,1,f_1.html> (referer: None)\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 492, in connect\n    sock = self._connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 550, in _connect\n    raise err\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 538, in _connect\n    sock.connect(socket_address)\nTimeoutError: [Errno 60] Operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/suool/PycharmProjects/Guba/Guba/spiders/NewsSpider.py\", line 50, in parse\n    redis_connect.lpush('News:start_urls', news_url)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 1611, in lpush\n    return self.execute_command('LPUSH', name, *values)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 772, in execute_command\n    connection = pool.get_connection(command_name, **options)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 994, in get_connection\n    connection.connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 497, in connect\n    raise ConnectionError(self._error_message(e))\nredis.exceptions.ConnectionError: Error 60 connecting to 120.***.***.***:6379. Operation timed out.\n\n2019-06-06 19:23:02 [scrapy.extensions.logstats] INFO: Crawled 489 pages (at 44 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:24:02 [scrapy.extensions.logstats] INFO: Crawled 540 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:25:02 [scrapy.extensions.logstats] INFO: Crawled 589 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:26:02 [scrapy.extensions.logstats] INFO: Crawled 638 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:27:02 [scrapy.extensions.logstats] INFO: Crawled 682 pages (at 44 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:28:02 [scrapy.extensions.logstats] INFO: Crawled 731 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:29:02 [scrapy.extensions.logstats] INFO: Crawled 781 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:30:02 [scrapy.extensions.logstats] INFO: Crawled 827 pages (at 46 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:31:02 [scrapy.extensions.logstats] INFO: Crawled 878 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:32:02 [scrapy.extensions.logstats] INFO: Crawled 931 pages (at 53 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:33:02 [scrapy.extensions.logstats] INFO: Crawled 981 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:34:02 [scrapy.extensions.logstats] INFO: Crawled 1032 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:35:02 [scrapy.extensions.logstats] INFO: Crawled 1081 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:36:02 [scrapy.extensions.logstats] INFO: Crawled 1130 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:37:02 [scrapy.extensions.logstats] INFO: Crawled 1178 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:38:02 [scrapy.extensions.logstats] INFO: Crawled 1225 pages (at 47 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:39:02 [scrapy.extensions.logstats] INFO: Crawled 1272 pages (at 47 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:40:02 [scrapy.extensions.logstats] INFO: Crawled 1322 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:41:02 [scrapy.extensions.logstats] INFO: Crawled 1371 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:42:02 [scrapy.extensions.logstats] INFO: Crawled 1421 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:43:02 [scrapy.extensions.logstats] INFO: Crawled 1471 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:44:02 [scrapy.extensions.logstats] INFO: Crawled 1519 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:45:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 4 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:46:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:47:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:48:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:49:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:50:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-06-06 19:50:46 [scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force \n2019-06-06 19:50:46 [scrapy.core.engine] INFO: Closing spider (shutdown)\n2019-06-06 19:50:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 474276,\n 'downloader/request_count': 1523,\n 'downloader/request_method_count/GET': 1523,",
    "tail": "2019-06-06 19:11:02 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: Guba)\n2019-06-06 19:11:02 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.6.0-x86_64-i386-64bit\n2019-06-06 19:11:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'Guba', 'CONCURRENT_REQUESTS': 8, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'logs/Guba/NewsUrlSpider/2019-06-06T19_10_54.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'Guba.spiders', 'ROBOTSTXT_OBEY': True, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['Guba.spiders']}\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2019-06-06 19:11:02 [NewsUrlSpider] INFO: Reading start URLs from redis key 'Guba:start_urls' (batch size: 8, encoding: utf-8\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'Guba.middlewares.RandomUserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-06-06 19:11:02 [scrapy.middleware] INFO: Enabled item pipelines:\n['Guba.pipelines.GubaPipeline']\n2019-06-06 19:11:02 [scrapy.core.engine] INFO: Spider opened\n\n2019-06-06 19:11:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:12:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:13:02 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 9 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:14:02 [scrapy.extensions.logstats] INFO: Crawled 54 pages (at 45 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:15:02 [scrapy.extensions.logstats] INFO: Crawled 105 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:16:02 [scrapy.extensions.logstats] INFO: Crawled 154 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:17:02 [scrapy.extensions.logstats] INFO: Crawled 204 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:18:02 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:19:02 [scrapy.extensions.logstats] INFO: Crawled 301 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:20:02 [scrapy.extensions.logstats] INFO: Crawled 349 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:21:02 [scrapy.extensions.logstats] INFO: Crawled 395 pages (at 46 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:22:02 [scrapy.extensions.logstats] INFO: Crawled 445 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n2019-06-06 19:22:42 [scrapy.core.scraper] ERROR: Spider error processing <GET http://guba.eastmoney.com/list,600578,1,f_1.html> (referer: None)\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 492, in connect\n    sock = self._connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 550, in _connect\n    raise err\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 538, in _connect\n    sock.connect(socket_address)\nTimeoutError: [Errno 60] Operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/suool/PycharmProjects/Guba/Guba/spiders/NewsSpider.py\", line 50, in parse\n    redis_connect.lpush('News:start_urls', news_url)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 1611, in lpush\n    return self.execute_command('LPUSH', name, *values)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 772, in execute_command\n    connection = pool.get_connection(command_name, **options)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 994, in get_connection\n    connection.connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 497, in connect\n    raise ConnectionError(self._error_message(e))\nredis.exceptions.ConnectionError: Error 60 connecting to 120.***.***.***:6379. Operation timed out.\n\n2019-06-06 19:23:02 [scrapy.extensions.logstats] INFO: Crawled 489 pages (at 44 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:24:02 [scrapy.extensions.logstats] INFO: Crawled 540 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:25:02 [scrapy.extensions.logstats] INFO: Crawled 589 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:26:02 [scrapy.extensions.logstats] INFO: Crawled 638 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:27:02 [scrapy.extensions.logstats] INFO: Crawled 682 pages (at 44 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:28:02 [scrapy.extensions.logstats] INFO: Crawled 731 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:29:02 [scrapy.extensions.logstats] INFO: Crawled 781 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:30:02 [scrapy.extensions.logstats] INFO: Crawled 827 pages (at 46 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:31:02 [scrapy.extensions.logstats] INFO: Crawled 878 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:32:02 [scrapy.extensions.logstats] INFO: Crawled 931 pages (at 53 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:33:02 [scrapy.extensions.logstats] INFO: Crawled 981 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:34:02 [scrapy.extensions.logstats] INFO: Crawled 1032 pages (at 51 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:35:02 [scrapy.extensions.logstats] INFO: Crawled 1081 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:36:02 [scrapy.extensions.logstats] INFO: Crawled 1130 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:37:02 [scrapy.extensions.logstats] INFO: Crawled 1178 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:38:02 [scrapy.extensions.logstats] INFO: Crawled 1225 pages (at 47 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:39:02 [scrapy.extensions.logstats] INFO: Crawled 1272 pages (at 47 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:40:02 [scrapy.extensions.logstats] INFO: Crawled 1322 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:41:02 [scrapy.extensions.logstats] INFO: Crawled 1371 pages (at 49 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:42:02 [scrapy.extensions.logstats] INFO: Crawled 1421 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:43:02 [scrapy.extensions.logstats] INFO: Crawled 1471 pages (at 50 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:44:02 [scrapy.extensions.logstats] INFO: Crawled 1519 pages (at 48 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:45:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 4 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:46:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:47:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:48:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:49:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n\n2019-06-06 19:50:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-06-06 19:50:46 [scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force \n2019-06-06 19:50:46 [scrapy.core.engine] INFO: Closing spider (shutdown)\n2019-06-06 19:50:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 474276,\n 'downloader/request_count': 1523,\n 'downloader/request_method_count/GET': 1523,\n 'downloader/response_bytes': 26185177,\n 'downloader/response_count': 1523,\n 'downloader/response_status_count/200': 1522,\n 'downloader/response_status_count/404': 1,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2019, 6, 6, 11, 50, 46, 545238),\n 'log_count/ERROR': 1,\n 'log_count/INFO': 48,\n 'memusage/max': 92938240,\n 'memusage/startup': 63553536,\n 'response_received_count': 1523,\n 'robotstxt/request_count': 1,\n 'robotstxt/response_count': 1,\n 'robotstxt/response_status_count/404': 1,\n 'scheduler/dequeued/redis': 1522,\n 'scheduler/enqueued/redis': 1522,\n 'spider_exceptions/ConnectionError': 1,\n 'start_time': datetime.datetime(2019, 6, 6, 11, 11, 2, 457544)}\n2019-06-06 19:50:46 [scrapy.core.engine] INFO: Spider closed (shutdown)\n",
    "first_log_timestamp": 1559819462,
    "latest_log_timestamp": 1559821846,
    "datas": [
        [
            "2019-06-06 19:11:02",
            0,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:12:02",
            0,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:13:02",
            9,
            9,
            0,
            0
        ],
        [
            "2019-06-06 19:14:02",
            54,
            45,
            0,
            0
        ],
        [
            "2019-06-06 19:15:02",
            105,
            51,
            0,
            0
        ],
        [
            "2019-06-06 19:16:02",
            154,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:17:02",
            204,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:18:02",
            252,
            48,
            0,
            0
        ],
        [
            "2019-06-06 19:19:02",
            301,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:20:02",
            349,
            48,
            0,
            0
        ],
        [
            "2019-06-06 19:21:02",
            395,
            46,
            0,
            0
        ],
        [
            "2019-06-06 19:22:02",
            445,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:23:02",
            489,
            44,
            0,
            0
        ],
        [
            "2019-06-06 19:24:02",
            540,
            51,
            0,
            0
        ],
        [
            "2019-06-06 19:25:02",
            589,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:26:02",
            638,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:27:02",
            682,
            44,
            0,
            0
        ],
        [
            "2019-06-06 19:28:02",
            731,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:29:02",
            781,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:30:02",
            827,
            46,
            0,
            0
        ],
        [
            "2019-06-06 19:31:02",
            878,
            51,
            0,
            0
        ],
        [
            "2019-06-06 19:32:02",
            931,
            53,
            0,
            0
        ],
        [
            "2019-06-06 19:33:02",
            981,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:34:02",
            1032,
            51,
            0,
            0
        ],
        [
            "2019-06-06 19:35:02",
            1081,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:36:02",
            1130,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:37:02",
            1178,
            48,
            0,
            0
        ],
        [
            "2019-06-06 19:38:02",
            1225,
            47,
            0,
            0
        ],
        [
            "2019-06-06 19:39:02",
            1272,
            47,
            0,
            0
        ],
        [
            "2019-06-06 19:40:02",
            1322,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:41:02",
            1371,
            49,
            0,
            0
        ],
        [
            "2019-06-06 19:42:02",
            1421,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:43:02",
            1471,
            50,
            0,
            0
        ],
        [
            "2019-06-06 19:44:02",
            1519,
            48,
            0,
            0
        ],
        [
            "2019-06-06 19:45:02",
            1523,
            4,
            0,
            0
        ],
        [
            "2019-06-06 19:46:02",
            1523,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:47:02",
            1523,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:48:02",
            1523,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:49:02",
            1523,
            0,
            0,
            0
        ],
        [
            "2019-06-06 19:50:02",
            1523,
            0,
            0,
            0
        ]
    ],
    "latest_matches": {
        "telnet_console": "",
        "resuming_crawl": "",
        "latest_offsite": "",
        "latest_duplicate": "",
        "latest_crawl": "",
        "latest_scrape": "",
        "latest_item": "",
        "latest_stat": "2019-06-06 19:50:02 [scrapy.extensions.logstats] INFO: Crawled 1523 pages (at 0 pages/min), scraped 0 items (at 0 items/min)"
    },
    "latest_crawl_timestamp": 0,
    "latest_scrape_timestamp": 0,
    "log_categories": {
        "critical_logs": {
            "count": 0,
            "details": []
        },
        "error_logs": {
            "count": 1,
            "details": [
                "2019-06-06 19:22:42 [scrapy.core.scraper] ERROR: Spider error processing <GET http://guba.eastmoney.com/list,600578,1,f_1.html> (referer: None)\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 492, in connect\n    sock = self._connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 550, in _connect\n    raise err\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 538, in _connect\n    sock.connect(socket_address)\nTimeoutError: [Errno 60] Operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/suool/PycharmProjects/Guba/Guba/spiders/NewsSpider.py\", line 50, in parse\n    redis_connect.lpush('News:start_urls', news_url)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 1611, in lpush\n    return self.execute_command('LPUSH', name, *values)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/client.py\", line 772, in execute_command\n    connection = pool.get_connection(command_name, **options)\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 994, in get_connection\n    connection.connect()\n  File \"/Users/suool/anaconda3/lib/python3.7/site-packages/redis/connection.py\", line 497, in connect\n    raise ConnectionError(self._error_message(e))\nredis.exceptions.ConnectionError: Error 60 connecting to 120.***.***.***:6379. Operation timed out."
            ]
        },
        "warning_logs": {
            "count": 0,
            "details": []
        },
        "redirect_logs": {
            "count": 0,
            "details": []
        },
        "retry_logs": {
            "count": 0,
            "details": []
        },
        "ignore_logs": {
            "count": 0,
            "details": []
        }
    },
    "crawler_stats": {
        "source": "log",
        "last_update_time": "2019-06-06 19:50:46",
        "last_update_timestamp": 1559821846,
        "downloader/request_bytes": 474276,
        "downloader/request_count": 1523,
        "downloader/request_method_count/GET": 1523,
        "downloader/response_bytes": 26185177,
        "downloader/response_count": 1523,
        "downloader/response_status_count/200": 1522,
        "downloader/response_status_count/404": 1,
        "finish_reason": "shutdown",
        "finish_time": "datetime.datetime(2019, 6, 6, 11, 50, 46, 545238)",
        "log_count/ERROR": 1,
        "log_count/INFO": 48,
        "memusage/max": 92938240,
        "memusage/startup": 63553536,
        "response_received_count": 1523,
        "robotstxt/request_count": 1,
        "robotstxt/response_count": 1,
        "robotstxt/response_status_count/404": 1,
        "scheduler/dequeued/redis": 1522,
        "scheduler/enqueued/redis": 1522,
        "spider_exceptions/ConnectionError": 1,
        "start_time": "datetime.datetime(2019, 6, 6, 11, 11, 2, 457544)"
    },
    "last_update_timestamp": 1559821885,
    "logparser_version": "0.8.1",
    "crawler_engine": {}
}