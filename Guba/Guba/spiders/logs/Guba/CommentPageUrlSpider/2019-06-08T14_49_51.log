2019-06-08 14:49:59 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: Guba)
2019-06-08 14:49:59 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.6.0-x86_64-i386-64bit
2019-06-08 14:49:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'Guba', 'CONCURRENT_REQUESTS': 2, 'DOWNLOAD_DELAY': 2, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_FILE': 'logs/Guba/CommentPageUrlSpider/2019-06-08T14_49_51.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'Guba.spiders', 'ROBOTSTXT_OBEY': True, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['Guba.spiders']}
2019-06-08 14:50:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2019-06-08 14:50:00 [CommentPageUrlSpider] INFO: Reading start URLs from redis key 'NewsTest:start_urls' (batch size: 2, encoding: utf-8
2019-06-08 14:50:10 [fake_useragent] WARNING: Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11
Traceback (most recent call last):
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 1317, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 1229, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 1275, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 1224, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 1016, in _send_output
    self.send(msg)
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 956, in send
    self.connect()
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 1384, in connect
    super().connect()
  File "/Users/suool/anaconda3/lib/python3.7/http/client.py", line 928, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/Users/suool/anaconda3/lib/python3.7/socket.py", line 727, in create_connection
    raise err
  File "/Users/suool/anaconda3/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/suool/anaconda3/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 1360, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/Users/suool/anaconda3/lib/python3.7/urllib/request.py", line 1319, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/suool/anaconda3/lib/python3.7/site-packages/fake_useragent/utils.py", line 154, in load
    for item in get_browsers(verify_ssl=verify_ssl):
  File "/Users/suool/anaconda3/lib/python3.7/site-packages/fake_useragent/utils.py", line 97, in get_browsers
    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)
  File "/Users/suool/anaconda3/lib/python3.7/site-packages/fake_useragent/utils.py", line 84, in get
    raise FakeUserAgentError('Maximum amount of retries reached')
fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached
2019-06-08 14:50:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Guba.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-06-08 14:50:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-06-08 14:50:11 [scrapy.middleware] INFO: Enabled item pipelines:
['Guba.pipelines.GubaPipeline']
2019-06-08 14:50:11 [scrapy.core.engine] INFO: Spider opened
2019-06-08 14:50:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-06-08 14:51:11 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 305 items (at 305 items/min)
2019-06-08 14:52:11 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 0 pages/min), scraped 305 items (at 0 items/min)
2019-06-08 14:53:11 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 9 pages/min), scraped 479 items (at 174 items/min)
2019-06-08 14:54:11 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 26 pages/min), scraped 932 items (at 453 items/min)
2019-06-08 14:54:52 [scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2019-06-08 14:54:52 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-06-08 14:54:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20336,
 'downloader/request_count': 65,
 'downloader/request_method_count/GET': 65,
 'downloader/response_bytes': 1767734,
 'downloader/response_count': 65,
 'downloader/response_status_count/200': 64,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2019, 6, 8, 6, 54, 54, 576022),
 'item_scraped_count': 1249,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'memusage/max': 81702912,
 'memusage/startup': 66416640,
 'response_received_count': 65,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued/redis': 64,
 'scheduler/enqueued/redis': 64,
 'start_time': datetime.datetime(2019, 6, 8, 6, 50, 11, 918744)}
2019-06-08 14:54:54 [scrapy.core.engine] INFO: Spider closed (shutdown)
